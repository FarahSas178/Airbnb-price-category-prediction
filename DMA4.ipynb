{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDQvEWD6CatP"
      },
      "source": [
        "#Probelm formulation\n",
        "This project is about posting a new listings on airbnb by checking similier posts with similar description for the apartment/house. So in this assignment, we are going to predict the listing price based on the listing characteristics , in this way to optimize user experience and lower the bar to be a new host. We have 2 input features which are reprensented in the image and the posts and two outputs which are the price and type, so we cut the pricing into three different bins for classification . For each listing, we recommend a pricing range to the new host rather than a fix price . So we define three categories: beginner, plus, premium based on the created listing. Respectively we use 0, 1, 2 to denote these three categories.\n",
        "\n",
        "The dataset contains listings of different areas in Montreal during 2019. It comes with rich information for each listing, including a link to the thumbnails etc. We will use a multi-objective (multi-task) multi-modality to predict our target.\n",
        "\n",
        "#### What data mining function is required?\n",
        "Prediction\n",
        "\n",
        "#### What could be the challenges?\n",
        "1)Preprocessing different types of data 2) Creating the best neural netwrok and choosing the right number of layers and the suitable paramters and optimzer 3)Dealing with two inputs and two outputs 4)Large dataset and incomplete data, text with different languages\n",
        "#### What is the impact? What is an ideal solution?\n",
        "People will be able to list their appartment with the exact right price and be able to sell their items easily. the ideal solution is in Trial 7 ussing VGG19 model\n",
        "\n",
        "#### Note: Please check the questions,the optimal solution and the reference by the end of the notebook "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing tools"
      ],
      "metadata": {
        "id": "WrEs4l4O1Ep1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "3jyAYRCMCVt1",
        "outputId": "96fa6294-feb6-476f-dd72-1e190844712b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-24 08:44:48--  https://github.com/CISC-873/Information-2021/releases/download/data/a4.zip\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/406495726/4d095bba-8b9b-4be4-8738-83f8ff5b0d18?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220324T084448Z&X-Amz-Expires=300&X-Amz-Signature=5756de91b05b70ff9c38317f2d179230f8774895e885f7a11916320c1a020f84&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=406495726&response-content-disposition=attachment%3B%20filename%3Da4.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-03-24 08:44:48--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/406495726/4d095bba-8b9b-4be4-8738-83f8ff5b0d18?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220324T084448Z&X-Amz-Expires=300&X-Amz-Signature=5756de91b05b70ff9c38317f2d179230f8774895e885f7a11916320c1a020f84&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=406495726&response-content-disposition=attachment%3B%20filename%3Da4.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 639078419 (609M) [application/octet-stream]\n",
            "Saving to: ‘a4.zip’\n",
            "\n",
            "a4.zip              100%[===================>] 609.47M  25.7MB/s    in 53s     \n",
            "\n",
            "2022-03-24 08:45:41 (11.5 MB/s) - ‘a4.zip’ saved [639078419/639078419]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                summary               image  \\\n",
              "0     Spacious, sunny and cozy modern apartment in t...     img_train/0.jpg   \n",
              "1     Located in one of the most vibrant and accessi...     img_train/1.jpg   \n",
              "2     Logement coquet et douillet à 10 minutes du ce...     img_train/2.jpg   \n",
              "3     Beautiful and spacious (1076 sc ft, / 100 mc) ...     img_train/3.jpg   \n",
              "4     Très grand appartement ''rustique'' et très ag...     img_train/4.jpg   \n",
              "...                                                 ...                 ...   \n",
              "7622  Un grand logement 4 et 1/2, tout inclut, bien ...  img_train/7626.jpg   \n",
              "7623  Magnificent condo directly on the river. You w...  img_train/7627.jpg   \n",
              "7624  This apartment is perfect for anyone visiting ...  img_train/7628.jpg   \n",
              "7625  It is a cozy ,clean ,and comfortable apartment...  img_train/7629.jpg   \n",
              "7626  Modern country style (newly-renovated); open c...  img_train/7630.jpg   \n",
              "\n",
              "           type  price  \n",
              "0     Apartment      1  \n",
              "1     Apartment      0  \n",
              "2     Apartment      1  \n",
              "3     Apartment      1  \n",
              "4     Apartment      0  \n",
              "...         ...    ...  \n",
              "7622  Apartment      0  \n",
              "7623  Apartment      2  \n",
              "7624  Apartment      1  \n",
              "7625  Apartment      0  \n",
              "7626      House      1  \n",
              "\n",
              "[7627 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aa506c9f-a6ec-4bd5-8703-55debada5708\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary</th>\n",
              "      <th>image</th>\n",
              "      <th>type</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Spacious, sunny and cozy modern apartment in t...</td>\n",
              "      <td>img_train/0.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Located in one of the most vibrant and accessi...</td>\n",
              "      <td>img_train/1.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Logement coquet et douillet à 10 minutes du ce...</td>\n",
              "      <td>img_train/2.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Beautiful and spacious (1076 sc ft, / 100 mc) ...</td>\n",
              "      <td>img_train/3.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Très grand appartement ''rustique'' et très ag...</td>\n",
              "      <td>img_train/4.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7622</th>\n",
              "      <td>Un grand logement 4 et 1/2, tout inclut, bien ...</td>\n",
              "      <td>img_train/7626.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7623</th>\n",
              "      <td>Magnificent condo directly on the river. You w...</td>\n",
              "      <td>img_train/7627.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7624</th>\n",
              "      <td>This apartment is perfect for anyone visiting ...</td>\n",
              "      <td>img_train/7628.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7625</th>\n",
              "      <td>It is a cozy ,clean ,and comfortable apartment...</td>\n",
              "      <td>img_train/7629.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7626</th>\n",
              "      <td>Modern country style (newly-renovated); open c...</td>\n",
              "      <td>img_train/7630.jpg</td>\n",
              "      <td>House</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7627 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa506c9f-a6ec-4bd5-8703-55debada5708')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aa506c9f-a6ec-4bd5-8703-55debada5708 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aa506c9f-a6ec-4bd5-8703-55debada5708');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
        "# from keras.optimizers import SGD\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "# Get file of data and download it \n",
        "! wget https://github.com/CISC-873/Information-2021/releases/download/data/a4.zip\n",
        "! unzip -q a4.zip\n",
        "# after unzipping the datset\n",
        "# we find that (through the file browser on the left) there is a csv file and a \n",
        "# folder of images\n",
        "df = pd.read_csv('/content/train_xy.csv')\n",
        "df1= pd.read_csv('/content/test_x.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPN8REp9GROl"
      },
      "outputs": [],
      "source": [
        "# !pip install googletrans==3.1.0\n",
        "# !pip install deep-translator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Zd1J-EwAETNW",
        "outputId": "de987342-bc9f-4a5b-b922-529417ae21d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                            summary  \\\n",
              "0        0  Charming warm house is ready to host you here ...   \n",
              "1        1  La chambre est spacieuse et lumineuse, dans un...   \n",
              "2        2  Grande chambre confortable située au sous-sol ...   \n",
              "3        3  Près d’un Métro, ligne orange. 10 minutes à pi...   \n",
              "4        4  Very bright appartment and very cosy. 2 separa...   \n",
              "...    ...                                                ...   \n",
              "7355  7626  Large, fully-furnished flat with brick walls a...   \n",
              "7356  7627  Logement situé dans le haut d’un duplex. Vivez...   \n",
              "7357  7628  My place is close to parks, . My place is good...   \n",
              "7358  7629  *** For security reasons, I will prioritize gu...   \n",
              "7359  7630  Stay in an amazing area of Montreal! 5-7 min f...   \n",
              "\n",
              "                  image  \n",
              "0        img_test/0.jpg  \n",
              "1        img_test/1.jpg  \n",
              "2        img_test/2.jpg  \n",
              "3        img_test/3.jpg  \n",
              "4        img_test/4.jpg  \n",
              "...                 ...  \n",
              "7355  img_test/7627.jpg  \n",
              "7356  img_test/7628.jpg  \n",
              "7357  img_test/7629.jpg  \n",
              "7358  img_test/7630.jpg  \n",
              "7359  img_test/7631.jpg  \n",
              "\n",
              "[7360 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-175ad902-8a4d-4ed2-b755-56bcc585d7cc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>summary</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Charming warm house is ready to host you here ...</td>\n",
              "      <td>img_test/0.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>La chambre est spacieuse et lumineuse, dans un...</td>\n",
              "      <td>img_test/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Grande chambre confortable située au sous-sol ...</td>\n",
              "      <td>img_test/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Près d’un Métro, ligne orange. 10 minutes à pi...</td>\n",
              "      <td>img_test/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Very bright appartment and very cosy. 2 separa...</td>\n",
              "      <td>img_test/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7355</th>\n",
              "      <td>7626</td>\n",
              "      <td>Large, fully-furnished flat with brick walls a...</td>\n",
              "      <td>img_test/7627.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7356</th>\n",
              "      <td>7627</td>\n",
              "      <td>Logement situé dans le haut d’un duplex. Vivez...</td>\n",
              "      <td>img_test/7628.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7357</th>\n",
              "      <td>7628</td>\n",
              "      <td>My place is close to parks, . My place is good...</td>\n",
              "      <td>img_test/7629.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7358</th>\n",
              "      <td>7629</td>\n",
              "      <td>*** For security reasons, I will prioritize gu...</td>\n",
              "      <td>img_test/7630.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7359</th>\n",
              "      <td>7630</td>\n",
              "      <td>Stay in an amazing area of Montreal! 5-7 min f...</td>\n",
              "      <td>img_test/7631.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7360 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-175ad902-8a4d-4ed2-b755-56bcc585d7cc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-175ad902-8a4d-4ed2-b755-56bcc585d7cc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-175ad902-8a4d-4ed2-b755-56bcc585d7cc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5in-GgoEei-"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# labels:\n",
        "df['type'] = df.type.astype('category').cat.codes\n",
        "\n",
        "len_type = len(df.type.unique())\n",
        "\n",
        "\n",
        "len_price = len(df.price.unique())\n",
        "# get the total number of unique outputs (later used for prediction)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KScMy1TGMN-"
      },
      "source": [
        "# Data Preprocessing\n",
        "We have image and text data.\n",
        "\n",
        "Image data: resize\n",
        "Text data: tokenization and converting to integer IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "efaa064112d943009b374814a6b064d5",
            "917fbdd5fb5847a6ae1ca33c646eabb0",
            "fadb0680530a4175a72f395ee88d43f5",
            "b4d8a32401844d228be5f391f6203417",
            "06391be42db445c2989f5fc529694d69",
            "b4d2a97ce9ee4adcb9ddaf812d88cc1b",
            "b17822ad910c48758b789146ab57a0ee",
            "ae73e39263344b8392afb1d95e0232b0",
            "4c8831e428424a538647f9c30c98d5aa",
            "cbc7de0761534ca9985a8e903e39b6e5",
            "0cee21df36b6491eb579b8209af08ed2",
            "25e9d2972e5e45498093b16b75a8097a",
            "3225db3151034fdbae67d1d48a6f5998",
            "495e813760d54697b3a83477ef6d6d7c",
            "a56ec038f22047878ce7ab6864592c38",
            "907838a6574d46d7a5f5991d3f6e9c52",
            "8e27d67c854d4de5b4a1b20489ed2177",
            "cab6fc7bc401417d83709f7dbab8b124",
            "521fb36963f64de28df1a5e13ed503e1",
            "2cbfa36ef0ad46b8b3a121f7cd325975",
            "1b58a292b2254aff95dc16962d4a3fcc",
            "4054da212a164ba9ae27b410db3f3c62"
          ]
        },
        "id": "gqY4JHFZF9bX",
        "outputId": "965b26f7-b0e8-473f-8e6e-8f12d0d9babb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7627 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efaa064112d943009b374814a6b064d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7360 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25e9d2972e5e45498093b16b75a8097a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# preprocess image data \n",
        "import os\n",
        "\n",
        "def load_image(file):\n",
        "    try:\n",
        "        image = Image.open(\n",
        "            file\n",
        "        ).convert('LA').resize((64, 64))\n",
        "        arr = np.array(image)\n",
        "    except:\n",
        "        arr = np.zeros((64, 64, 2))\n",
        "    return arr\n",
        "\n",
        "\n",
        "# loading images:\n",
        "x_train_image = np.array([load_image(i) for i in tqdm(df.image)])\n",
        "x_test_image = np.array([load_image(i) for i in tqdm(df1.image)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRxdpQPoGROo"
      },
      "outputs": [],
      "source": [
        "# Definig our labels\n",
        "y_train_type = df.type\n",
        "\n",
        "\n",
        "y_train_price = df.price"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading translator\n",
        "!pip install googletrans==3.1.0a0\n",
        "!pip install deep-translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5sp3jV_IE85",
        "outputId": "d31e3832-7ce7-4be7-a446-4601eb49e30f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting hstspreload\n",
            "  Downloading hstspreload-2021.12.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 31.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.10.8)\n",
            "Collecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.0 MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16367 sha256=a6eae36bf0f2a80d5d871f8e0b3777f3ac12f24cfbcf0fd3290b698136ae3f4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/be/fe/93a6a40ffe386e16089e44dad9018ebab9dc4cb9eb7eab65ae\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hyperframe, hpack, sniffio, h2, h11, rfc3986, httpcore, hstspreload, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2021.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.8.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from deep-translator) (2.23.0)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.9.1\n",
            "  Downloading beautifulsoup4-4.10.0-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.3.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2021.10.8)\n",
            "Installing collected packages: beautifulsoup4, deep-translator\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed beautifulsoup4-4.10.0 deep-translator-1.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0Z1e-2jGROo"
      },
      "outputs": [],
      "source": [
        "from googletrans import Translator\n",
        "translator = Translator()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MMbp7reJAt_"
      },
      "outputs": [],
      "source": [
        "# loading summary: (force convert some of the non-string cell to string)\n",
        "x_train_text = df.summary.astype('str')\n",
        "x_test_text = df1.summary.astype('str')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "C9gicr98J_ww",
        "outputId": "35f14d82-71cf-4377-ca25-bfee16a72f4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7627, 64, 64, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# check image loading\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_train_image[100, :, :, 0])\n",
        "x_train_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8UuOVIwGROq",
        "outputId": "8e30f8af-eb68-407c-d067-8edd107309b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7627, 100)\n"
          ]
        }
      ],
      "source": [
        "# preprocess text data\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pprint import pprint\n",
        "\n",
        "vocab_size = 40000\n",
        "max_len = 100\n",
        "\n",
        "\n",
        "# build vocabulary from training set\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(x_train_text)\n",
        "\n",
        "\n",
        "def _preprocess(list_of_text):\n",
        "    return pad_sequences(\n",
        "        tokenizer.texts_to_sequences(list_of_text),\n",
        "        maxlen=max_len,\n",
        "        padding='post',\n",
        "    )\n",
        "    \n",
        "\n",
        "# padding is done inside: \n",
        "x_train_text_id = _preprocess(x_train_text)\n",
        "\n",
        "print(x_train_text_id.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-U_zFpOGROr",
        "outputId": "9515f800-3b66-4fee-f883-b00cecb32680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7360, 100)\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(x_test_text)\n",
        "x_test_text_id = _preprocess(x_test_text)\n",
        "\n",
        "print(x_test_text_id.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm6sBwyGGROr",
        "outputId": "b7efbf2d-2d8b-468a-ff0c-b621169a088a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['building hotel and enjoy 15 apartment in the are of montreal close shops '\n",
            " 'will character located in the street separate 5 mont kitchen cozy in the '\n",
            " 'museums of mattresses numerous going only very all to this travelers '\n",
            " 'downtown double wifi one self right sun modern lots closed montreal stations '\n",
            " 'a off single 2000 st in the are of the standard thank board métro record '\n",
            " 'need included triplex milton buildings and catherine ride charming',\n",
            " 'located in living of the little laundry and lively classy of minutes '\n",
            " 'montreal close living will available or front only personalized you view pet '\n",
            " 'you with living of the little feeling royale it is an distance of the '\n",
            " 'separate cat dryer very the me free the parking port streets jarry maxi '\n",
            " 'metro and ride ride charming',\n",
            " 'pharmacies and enjoy subway can 2 from minutes montreal stay ideal rosemont '\n",
            " 'relaxing downtown and jean around sofa area please i 4 bathroom le des jazz',\n",
            " 'min and building dézéry manners neighborhoods visiting court available walk '\n",
            " 'the peu be in the peace takes of montreal located in a by we’ve have walking '\n",
            " 'a jacques of it’s like phone shops my experience from better tour sleep '\n",
            " 'concordia my from across campus uqam pax clean my from the netflix couple my '\n",
            " 'room from 普通话 12 place 4 choose minutes tourist in nearby my and is all to '\n",
            " 'the min westmount private reserve uqam 同埋广东话｡ room and loft french in out '\n",
            " 'and seeing and field joe present french in stays',\n",
            " 'street quiet t1 and street swimming apartment for main in a nice cozy of '\n",
            " 'montreal a quiet will in the ground and walk the each be best is a will a '\n",
            " 'quiet at solo on and a quiet mile bus on the at sherbrooke few cross parts '\n",
            " 'equipped a sedentary a botanical igloo a double ix a boiling gym a creating '\n",
            " 'gym street all to the renovating and minutes 1st and montreal walk town your '\n",
            " 'metro loft bedroom old']\n"
          ]
        }
      ],
      "source": [
        "# we can use the tokenizer to convert IDs to words.\n",
        "pprint(tokenizer.sequences_to_texts(x_train_text_id[:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGINMu1gGROs",
        "outputId": "2288c6b6-b7d4-4996-dc03-68ae118d7f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['charming warm house is ready to host you here in multi cultural parc ex '\n",
            " 'short walk to parc jarry to watch rogers cup and on major bus lines to metro '\n",
            " 'and downtown',\n",
            " 'the room is spacious and bright in an apartment shared with three other '\n",
            " 'roommates',\n",
            " 'large comfortable room located in the basement of our house a large bathroom '\n",
            " 'with shower washer and dryer are included',\n",
            " 'near a metro orange line 10 minutes walking ahuntsic residential area of '\n",
            " '\\u200b\\u200bmontreal north center easy street parking one bedroom double bed '\n",
            " '2 people maximum during my presence 1 bedroom and 2 bedrooms during my '\n",
            " 'absence friendly and warm',\n",
            " 'very bright appartment and very cosy 2 separate bedrooms 1 with a queen size '\n",
            " 'and the other with a double sofa bed located in verdun near promenade '\n",
            " 'wellington and its nice bars and restaurants']\n"
          ]
        }
      ],
      "source": [
        "# we can use the tokenizer to convert IDs to words.\n",
        "pprint(tokenizer.sequences_to_texts(x_test_text_id[:5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see there are some words in french and other different languages than english"
      ],
      "metadata": {
        "id": "OgcPt1oK19IT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCdB-aXRGROp"
      },
      "outputs": [],
      "source": [
        "#Translating train dataset into english\n",
        "x_train_text_id= x_train_text_id.apply(lambda x: translator.translate(x, dest='en').text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWiXne_-GROp"
      },
      "outputs": [],
      "source": [
        "#Translating test dataset into english\n",
        "x_test_text_id= x_test_text_id.apply(lambda x: translator.translate(x, dest='en').text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvBsuhNXGROs",
        "outputId": "616cff96-2085-45fc-d76c-317639a18051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total words in the dictionary: 40000\n"
          ]
        }
      ],
      "source": [
        "print('total words in the dictionary:', tokenizer.num_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by_Vu26qGROt"
      },
      "source": [
        "# Building a Learning model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trial 1"
      ],
      "metadata": {
        "id": "pzuMWlvEv7gD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will tune the hyperparameters manually by try and error"
      ],
      "metadata": {
        "id": "mXZ555oCfAfV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48XiHxfoGROt",
        "outputId": "1259a4af-3941-4322-a889-b636d1eed8e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 49, 49, 32)   16416       ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 100, 100)     4000000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 3, 3, 32)     0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpLambd  (None, 100)         0           ['embedding[0][0]']              \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 288)          0           ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " tf.concat (TFOpLambda)         (None, 388)          0           ['tf.math.reduce_mean[0][0]',    \n",
            "                                                                  'flatten[0][0]']                \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            1167        ['tf.concat[0][0]']              \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           9336        ['tf.concat[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,026,919\n",
            "Trainable params: 4,026,919\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# here we have two inputs. one for image and the other for text.\n",
        "in_text = keras.Input(batch_shape=(None, max_len))\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "# text part\n",
        "# simple average of embedding. you can change it to anything else as needed\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
        "averaged = tf.reduce_mean(embedded, axis=1)\n",
        "\n",
        "\n",
        "# image part \n",
        "# simple conv2d. you can change it to anything else as needed\n",
        "cov = Conv2D(32, (16, 16))(in_image)\n",
        "pl = MaxPool2D((16, 16))(cov)\n",
        "flattened = Flatten()(pl)\n",
        "\n",
        "\n",
        "# fusion - combinig both\n",
        "fused = tf.concat([averaged, flattened], axis=-1)\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "# define model input/output using keys.\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summery': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.5,\n",
        "        'price': 0.5,       \n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7410bcbf-0ee5-44a8-af1b-bdb8d889b41d",
        "id": "iRMkuW07pM1j"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "382/382 [==============================] - 6s 14ms/step - loss: 25.7387 - price_loss: 19.2028 - type_loss: 32.2745 - price_sparse_categorical_accuracy: 0.4971 - type_sparse_categorical_accuracy: 0.5771 - val_loss: 10.0933 - val_price_loss: 5.3569 - val_type_loss: 14.8297 - val_price_sparse_categorical_accuracy: 0.5138 - val_type_sparse_categorical_accuracy: 0.2864\n",
            "Epoch 2/20\n",
            "382/382 [==============================] - 5s 13ms/step - loss: 7.6929 - price_loss: 5.3741 - type_loss: 10.0118 - price_sparse_categorical_accuracy: 0.5171 - type_sparse_categorical_accuracy: 0.5906 - val_loss: 5.5616 - val_price_loss: 4.4297 - val_type_loss: 6.6936 - val_price_sparse_categorical_accuracy: 0.5865 - val_type_sparse_categorical_accuracy: 0.6153\n",
            "Epoch 3/20\n",
            "382/382 [==============================] - 5s 12ms/step - loss: 5.0071 - price_loss: 3.7830 - type_loss: 6.2311 - price_sparse_categorical_accuracy: 0.5379 - type_sparse_categorical_accuracy: 0.5892 - val_loss: 4.6129 - val_price_loss: 3.5777 - val_type_loss: 5.6481 - val_price_sparse_categorical_accuracy: 0.5675 - val_type_sparse_categorical_accuracy: 0.5177\n",
            "Epoch 4/20\n",
            "382/382 [==============================] - 5s 12ms/step - loss: 9.1853 - price_loss: 7.1689 - type_loss: 11.2018 - price_sparse_categorical_accuracy: 0.5307 - type_sparse_categorical_accuracy: 0.5935 - val_loss: 6.7963 - val_price_loss: 4.4923 - val_type_loss: 9.1003 - val_price_sparse_categorical_accuracy: 0.4430 - val_type_sparse_categorical_accuracy: 0.7228\n",
            "Epoch 5/20\n",
            "382/382 [==============================] - 5s 12ms/step - loss: 7.6766 - price_loss: 4.7041 - type_loss: 10.6491 - price_sparse_categorical_accuracy: 0.5665 - type_sparse_categorical_accuracy: 0.6027 - val_loss: 5.0110 - val_price_loss: 3.9731 - val_type_loss: 6.0490 - val_price_sparse_categorical_accuracy: 0.5308 - val_type_sparse_categorical_accuracy: 0.7195\n",
            "Epoch 6/20\n",
            "382/382 [==============================] - 5s 13ms/step - loss: 7.8914 - price_loss: 5.2126 - type_loss: 10.5702 - price_sparse_categorical_accuracy: 0.5655 - type_sparse_categorical_accuracy: 0.6066 - val_loss: 5.1307 - val_price_loss: 3.5953 - val_type_loss: 6.6661 - val_price_sparse_categorical_accuracy: 0.5360 - val_type_sparse_categorical_accuracy: 0.7097\n",
            "Epoch 7/20\n",
            "382/382 [==============================] - 5s 12ms/step - loss: 11.9074 - price_loss: 7.5789 - type_loss: 16.2358 - price_sparse_categorical_accuracy: 0.5638 - type_sparse_categorical_accuracy: 0.6042 - val_loss: 5.0542 - val_price_loss: 3.7746 - val_type_loss: 6.3338 - val_price_sparse_categorical_accuracy: 0.5931 - val_type_sparse_categorical_accuracy: 0.6363\n",
            "Epoch 8/20\n",
            "382/382 [==============================] - 5s 13ms/step - loss: 11.9313 - price_loss: 7.2451 - type_loss: 16.6175 - price_sparse_categorical_accuracy: 0.5835 - type_sparse_categorical_accuracy: 0.6110 - val_loss: 6.9797 - val_price_loss: 4.7109 - val_type_loss: 9.2485 - val_price_sparse_categorical_accuracy: 0.5806 - val_type_sparse_categorical_accuracy: 0.5904\n",
            "Epoch 9/20\n",
            "382/382 [==============================] - 5s 12ms/step - loss: 10.5224 - price_loss: 5.6952 - type_loss: 15.3497 - price_sparse_categorical_accuracy: 0.6029 - type_sparse_categorical_accuracy: 0.6201 - val_loss: 26.4129 - val_price_loss: 9.8930 - val_type_loss: 42.9329 - val_price_sparse_categorical_accuracy: 0.4325 - val_type_sparse_categorical_accuracy: 0.7634\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from numpy.random import seed\n",
        "seed(3)\n",
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=6 )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f5384db-bc72-496a-a801-eac0f3717d66",
        "id": "ZUUt5kAmpUv1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3.5444529e-13 1.0000000e+00 1.9608071e-13]\n",
            " [4.7983466e-08 1.0000000e+00 3.6238160e-14]\n",
            " [9.9506652e-01 4.9334439e-03 3.6739756e-17]\n",
            " ...\n",
            " [9.9999642e-01 3.5664586e-06 1.0203645e-17]\n",
            " [4.9428537e-02 9.5057148e-01 3.6602530e-16]\n",
            " [6.2914296e-10 1.0000000e+00 9.0492622e-21]]\n",
            "[1 1 0 ... 0 1 1]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted = y_predict['price']\n",
        "print(price_predicted)\n",
        "\n",
        "# categories\n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "print(price_category_predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F7rNW4VpUv3"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': df1.index,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('sample_submissionTrial1.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Public score: 0.43695\n",
        "0.44103\n"
      ],
      "metadata": {
        "id": "toeCrjbAv_JC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see we have large value of loss and there is overfitting \n",
        "We will try different values of epoch, batch size,validation size and try LSTM in the next trials"
      ],
      "metadata": {
        "id": "sl34CTgV2LM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trial 2"
      ],
      "metadata": {
        "id": "4JirRCQYp2aj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM stands for Long-Short Term Memory. LSTM is a type of recurrent neural network but is better than traditional recurrent neural networks in terms of memory. Having a good hold over memorizing certain patterns LSTMs perform fairly better. As with every other NN, LSTM can have multiple hidden layers and as it passes through every layer, the relevant information is kept and all the irrelevant information gets discarded in every single cell.\n",
        "LSTM has 3 main gates.\n",
        "\n",
        "1. FORGET Gate\n",
        "2. INPUT Gate\n",
        "3. OUTPUT Gate\n",
        "LSTM is that it is effective in memorizing important information.\n",
        "\n",
        "In LSTM we can use a multiple word string to find out the class to which it belongs. This is very helpful while working with Natural language processing. "
      ],
      "metadata": {
        "id": "VQgG5JmS3EWs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwAPaIoQGROu",
        "outputId": "919b4169-ca71-4f2d-b540-6aaa62880599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 49, 49, 32)   16416       ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 100, 100)     4000000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 3, 3, 32)     0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 100)          80400       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 288)          0           ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " tf.concat (TFOpLambda)         (None, 388)          0           ['lstm[0][0]',                   \n",
            "                                                                  'flatten[0][0]']                \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            1167        ['tf.concat[0][0]']              \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           9336        ['tf.concat[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,107,319\n",
            "Trainable params: 4,107,319\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# image part \n",
        "# simple conv2d. you can change it to anything else as needed\n",
        "cov = Conv2D(32, (16, 16))(in_image)\n",
        "pl = MaxPool2D((16, 16))(cov)\n",
        "flattened = Flatten()(pl)\n",
        "\n",
        "# word part\n",
        "w=LSTM(100)(embedded)\n",
        "\n",
        "\n",
        "\n",
        "# fusion - combinig both\n",
        "fused = tf.concat([w, flattened], axis=-1)\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "# define model input/output using keys.\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summery': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.7,\n",
        "        'price': 0.3,       \n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6XkGCsRGROv"
      },
      "source": [
        "# Model Training\n",
        "Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "febe3ad8-8bf5-4b31-8037-becebf6e67a5",
        "id": "j0zWtUtjrohe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "534/534 [==============================] - 23s 34ms/step - loss: 22.8742 - price_loss: 17.6338 - type_loss: 28.1147 - price_sparse_categorical_accuracy: 0.4863 - type_sparse_categorical_accuracy: 0.5811 - val_loss: 17.0997 - val_price_loss: 8.1180 - val_type_loss: 26.0815 - val_price_sparse_categorical_accuracy: 0.3517 - val_type_sparse_categorical_accuracy: 0.6094\n",
            "Epoch 2/55\n",
            "534/534 [==============================] - 16s 29ms/step - loss: 9.2287 - price_loss: 6.3556 - type_loss: 12.1018 - price_sparse_categorical_accuracy: 0.5133 - type_sparse_categorical_accuracy: 0.5877 - val_loss: 9.2881 - val_price_loss: 7.6357 - val_type_loss: 10.9405 - val_price_sparse_categorical_accuracy: 0.4893 - val_type_sparse_categorical_accuracy: 0.3914\n",
            "Epoch 3/55\n",
            "534/534 [==============================] - 15s 27ms/step - loss: 9.9748 - price_loss: 7.4422 - type_loss: 12.5075 - price_sparse_categorical_accuracy: 0.4949 - type_sparse_categorical_accuracy: 0.5845 - val_loss: 7.7340 - val_price_loss: 4.7424 - val_type_loss: 10.7256 - val_price_sparse_categorical_accuracy: 0.5173 - val_type_sparse_categorical_accuracy: 0.6972\n",
            "Epoch 4/55\n",
            "534/534 [==============================] - 15s 27ms/step - loss: 8.3415 - price_loss: 6.5487 - type_loss: 10.1343 - price_sparse_categorical_accuracy: 0.5139 - type_sparse_categorical_accuracy: 0.5912 - val_loss: 8.0329 - val_price_loss: 5.8926 - val_type_loss: 10.1732 - val_price_sparse_categorical_accuracy: 0.5609 - val_type_sparse_categorical_accuracy: 0.5177\n",
            "Epoch 5/55\n",
            "534/534 [==============================] - 15s 28ms/step - loss: 8.8446 - price_loss: 6.8556 - type_loss: 10.8336 - price_sparse_categorical_accuracy: 0.5118 - type_sparse_categorical_accuracy: 0.5875 - val_loss: 11.2428 - val_price_loss: 6.0951 - val_type_loss: 16.3905 - val_price_sparse_categorical_accuracy: 0.5151 - val_type_sparse_categorical_accuracy: 0.2774\n",
            "Epoch 6/55\n",
            "534/534 [==============================] - 16s 29ms/step - loss: 16.0444 - price_loss: 11.5390 - type_loss: 20.5497 - price_sparse_categorical_accuracy: 0.5007 - type_sparse_categorical_accuracy: 0.5802 - val_loss: 10.7528 - val_price_loss: 5.1321 - val_type_loss: 16.3734 - val_price_sparse_categorical_accuracy: 0.4823 - val_type_sparse_categorical_accuracy: 0.7029\n",
            "Epoch 7/55\n",
            "534/534 [==============================] - 15s 28ms/step - loss: 9.7620 - price_loss: 6.8241 - type_loss: 12.6999 - price_sparse_categorical_accuracy: 0.5214 - type_sparse_categorical_accuracy: 0.6019 - val_loss: 19.1715 - val_price_loss: 15.0074 - val_type_loss: 23.3355 - val_price_sparse_categorical_accuracy: 0.3818 - val_type_sparse_categorical_accuracy: 0.6732\n",
            "Epoch 8/55\n",
            "534/534 [==============================] - 15s 27ms/step - loss: 10.0978 - price_loss: 7.1409 - type_loss: 13.0548 - price_sparse_categorical_accuracy: 0.5266 - type_sparse_categorical_accuracy: 0.5984 - val_loss: 11.5003 - val_price_loss: 7.2671 - val_type_loss: 15.7335 - val_price_sparse_categorical_accuracy: 0.4159 - val_type_sparse_categorical_accuracy: 0.6466\n",
            "Epoch 9/55\n",
            "534/534 [==============================] - 15s 27ms/step - loss: 10.7376 - price_loss: 7.6562 - type_loss: 13.8189 - price_sparse_categorical_accuracy: 0.5249 - type_sparse_categorical_accuracy: 0.6060 - val_loss: 12.1029 - val_price_loss: 6.2866 - val_type_loss: 17.9193 - val_price_sparse_categorical_accuracy: 0.5614 - val_type_sparse_categorical_accuracy: 0.7409\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from numpy.random import seed\n",
        "seed(3)\n",
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=55,\n",
        "    batch_size=10,\n",
        "    validation_split=0.3,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=6 )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoM0fu_mGROv"
      },
      "source": [
        "# Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2415c680-1de8-4743-c428-42f8cc5b9d2d",
        "id": "oTvqdauPryTJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9.9999964e-01 3.0402140e-07 1.8282108e-19]\n",
            " [9.1846883e-01 8.1531174e-02 1.6384799e-21]\n",
            " [9.9943584e-01 4.3292282e-08 5.6411797e-04]\n",
            " ...\n",
            " [9.9999845e-01 1.5888977e-06 1.3811070e-31]\n",
            " [9.4193697e-01 5.8063019e-02 1.4869862e-10]\n",
            " [4.6886826e-01 5.3113174e-01 3.6169878e-18]]\n",
            "[0 0 0 ... 0 0 1]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted = y_predict['price']\n",
        "print(price_predicted)\n",
        "\n",
        "# categories\n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "print(price_category_predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHI7Urs6ryTK"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': df1.index,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('sample_submissionTrial2.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Private :0.55896\n",
        "Public: 0.55516"
      ],
      "metadata": {
        "id": "FZ_RUJ6BwICs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that the performance of the model has improved but still there is overfiiting we will see how can we solve it but lets try using Bidirectional layer with LSTM"
      ],
      "metadata": {
        "id": "fr2iCoBr4f7D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgHafnIqGROw"
      },
      "source": [
        "# Trial 3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bidirectional recurrent neural networks (BRNN) connect two hidden layers of opposite directions to the same output. With this form of generative deep learning, the output layer can get information from past (backwards) and future (forward) states simultaneously.Which helps in improving the performance of our model"
      ],
      "metadata": {
        "id": "EcfQwrtj48VE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFNw3n7FGROw",
        "outputId": "35730ebf-d7f0-47af-dd96-da218635409d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 49, 49, 32)   16416       ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 100, 100)     4000000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 32)    0           ['conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " gru (GRU)                      (None, 100)          60600       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)            (None, 288)          0           ['max_pooling2d_4[0][0]']        \n",
            "                                                                                                  \n",
            " tf.concat_4 (TFOpLambda)       (None, 388)          0           ['gru[0][0]',                    \n",
            "                                                                  'flatten_4[0][0]']              \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            1167        ['tf.concat_4[0][0]']            \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           9336        ['tf.concat_4[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,087,519\n",
            "Trainable params: 4,087,519\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Bidirectional\n",
        "\n",
        "# image part \n",
        "# simple conv2d. you can change it to anything else as needed\n",
        "cov = Conv2D(32, (16, 16))(in_image)\n",
        "pl = MaxPool2D((16, 16))(cov)\n",
        "\n",
        "flattened = Flatten()(pl)\n",
        "\n",
        "# word part\n",
        "w=Bidirectional(LSTM(100))(embedded)\n",
        "\n",
        "\n",
        "# fusion - combinig both\n",
        "fused = tf.concat([w, flattened], axis=-1)\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "# define model input/output using keys.\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summery': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.5,\n",
        "        'price': 0.5,       \n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8KANvj5GROx"
      },
      "source": [
        "# Model Training\n",
        "Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "949ce3fb-66cd-4fd0-facd-47e7ccf4b008",
        "id": "kMR4JsIIndLB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "534/534 [==============================] - 40s 67ms/step - loss: 19.5023 - price_loss: 15.7732 - type_loss: 23.2314 - price_sparse_categorical_accuracy: 0.5199 - type_sparse_categorical_accuracy: 0.5824 - val_loss: 7.0979 - val_price_loss: 3.7123 - val_type_loss: 10.4835 - val_price_sparse_categorical_accuracy: 0.5872 - val_type_sparse_categorical_accuracy: 0.6868\n",
            "Epoch 2/55\n",
            "534/534 [==============================] - 35s 66ms/step - loss: 7.1384 - price_loss: 5.1554 - type_loss: 9.1214 - price_sparse_categorical_accuracy: 0.5712 - type_sparse_categorical_accuracy: 0.6040 - val_loss: 5.8038 - val_price_loss: 5.0220 - val_type_loss: 6.5856 - val_price_sparse_categorical_accuracy: 0.5159 - val_type_sparse_categorical_accuracy: 0.6099\n",
            "Epoch 3/55\n",
            "534/534 [==============================] - 35s 65ms/step - loss: 8.4765 - price_loss: 5.6945 - type_loss: 11.2585 - price_sparse_categorical_accuracy: 0.6043 - type_sparse_categorical_accuracy: 0.6195 - val_loss: 15.6632 - val_price_loss: 16.7495 - val_type_loss: 14.5769 - val_price_sparse_categorical_accuracy: 0.2219 - val_type_sparse_categorical_accuracy: 0.6789\n",
            "Epoch 4/55\n",
            "534/534 [==============================] - 35s 65ms/step - loss: 9.9546 - price_loss: 6.6747 - type_loss: 13.2344 - price_sparse_categorical_accuracy: 0.6214 - type_sparse_categorical_accuracy: 0.6379 - val_loss: 15.7679 - val_price_loss: 11.4602 - val_type_loss: 20.0757 - val_price_sparse_categorical_accuracy: 0.6448 - val_type_sparse_categorical_accuracy: 0.7440\n",
            "Epoch 5/55\n",
            "534/534 [==============================] - 35s 65ms/step - loss: 11.1472 - price_loss: 7.8582 - type_loss: 14.4363 - price_sparse_categorical_accuracy: 0.6270 - type_sparse_categorical_accuracy: 0.6650 - val_loss: 10.1989 - val_price_loss: 6.7101 - val_type_loss: 13.6877 - val_price_sparse_categorical_accuracy: 0.6313 - val_type_sparse_categorical_accuracy: 0.6697\n",
            "Epoch 6/55\n",
            "534/534 [==============================] - 35s 65ms/step - loss: 10.1680 - price_loss: 6.3572 - type_loss: 13.9787 - price_sparse_categorical_accuracy: 0.6508 - type_sparse_categorical_accuracy: 0.6615 - val_loss: 13.8845 - val_price_loss: 7.3470 - val_type_loss: 20.4221 - val_price_sparse_categorical_accuracy: 0.6287 - val_type_sparse_categorical_accuracy: 0.7523\n",
            "Epoch 7/55\n",
            "534/534 [==============================] - 35s 65ms/step - loss: 13.3741 - price_loss: 9.2366 - type_loss: 17.5115 - price_sparse_categorical_accuracy: 0.6568 - type_sparse_categorical_accuracy: 0.6798 - val_loss: 7.7850 - val_price_loss: 5.2453 - val_type_loss: 10.3248 - val_price_sparse_categorical_accuracy: 0.6221 - val_type_sparse_categorical_accuracy: 0.7007\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from numpy.random import seed\n",
        "seed(3)\n",
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=55,\n",
        "    batch_size=10,\n",
        "    validation_split=0.3,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=6 )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmINqYZindLW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4K_s3MQGROx"
      },
      "source": [
        "# Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6c1a11-69cf-43ea-b709-9f7185b08597",
        "id": "vLVAF3ZyndLV"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0000000e+00 1.1836339e-10 3.0517439e-11]\n",
            " [1.0000000e+00 1.3006818e-10 1.3380564e-22]\n",
            " [1.0000000e+00 4.2877834e-20 1.5647402e-22]\n",
            " ...\n",
            " [1.0000000e+00 8.1688578e-10 5.0284821e-11]\n",
            " [1.0000000e+00 3.4892370e-08 2.0522582e-30]\n",
            " [9.1603342e-03 9.9083966e-01 1.0270705e-12]]\n",
            "[0 0 0 ... 0 0 1]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted = y_predict['price']\n",
        "print(price_predicted)\n",
        "\n",
        "# categories\n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "print(price_category_predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crdyIpYIndLW"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': df1.index,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('sample_submissionTrial3.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Public score: 0.60353\n",
        "0.62690"
      ],
      "metadata": {
        "id": "lK-zj4ZnwQbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe the improvement in our model.\n",
        "We will decrease the value of patience to try to overcome the overfitting"
      ],
      "metadata": {
        "id": "hRn2esxX5W9b"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjgqB_3UGROy"
      },
      "source": [
        "# Trial 4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will try GRU"
      ],
      "metadata": {
        "id": "W5cTt-W9-DMd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g3AYFY-GROy",
        "outputId": "8704532d-0d40-4c2d-fe92-b1525396d579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 49, 49, 32)   16416       ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 100, 100)     4000000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_6 (MaxPooling2D)  (None, 3, 3, 32)    0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " gru_2 (GRU)                    (None, 100)          60600       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_6 (Flatten)            (None, 288)          0           ['max_pooling2d_6[0][0]']        \n",
            "                                                                                                  \n",
            " tf.concat_6 (TFOpLambda)       (None, 388)          0           ['gru_2[0][0]',                  \n",
            "                                                                  'flatten_6[0][0]']              \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            1167        ['tf.concat_6[0][0]']            \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           9336        ['tf.concat_6[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,087,519\n",
            "Trainable params: 4,087,519\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Bidirectional\n",
        "\n",
        "# image part \n",
        "# simple conv2d. you can change it to anything else as needed\n",
        "cov = Conv2D(32, (16, 16))(in_image)\n",
        "\n",
        "pl = MaxPool2D((16, 16))(cov)\n",
        "\n",
        "flattened = Flatten()(pl)\n",
        "\n",
        "# word part\n",
        "w=GRU(100)(embedded)\n",
        "\n",
        "\n",
        "\n",
        "# fusion - combinig both\n",
        "fused = tf.concat([w, flattened], axis=-1)\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "# define model input/output using keys.\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summery': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.5,\n",
        "        'price': 0.5,       \n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlJE6qaRGROz"
      },
      "source": [
        "# Model Training\n",
        "Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a0c0994-cd10-48ba-d35f-3489cd68a299",
        "id": "34tBCR6VlRe-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "534/534 [==============================] - 27s 39ms/step - loss: 23.3063 - price_loss: 18.4509 - type_loss: 28.1617 - price_sparse_categorical_accuracy: 0.4953 - type_sparse_categorical_accuracy: 0.5811 - val_loss: 11.9867 - val_price_loss: 9.1738 - val_type_loss: 14.7995 - val_price_sparse_categorical_accuracy: 0.4045 - val_type_sparse_categorical_accuracy: 0.4788\n",
            "Epoch 2/55\n",
            "534/534 [==============================] - 18s 34ms/step - loss: 11.6615 - price_loss: 8.1289 - type_loss: 15.1940 - price_sparse_categorical_accuracy: 0.4987 - type_sparse_categorical_accuracy: 0.5824 - val_loss: 6.8348 - val_price_loss: 6.1728 - val_type_loss: 7.4968 - val_price_sparse_categorical_accuracy: 0.4788 - val_type_sparse_categorical_accuracy: 0.5046\n",
            "Epoch 3/55\n",
            "534/534 [==============================] - 18s 33ms/step - loss: 7.8175 - price_loss: 5.6382 - type_loss: 9.9968 - price_sparse_categorical_accuracy: 0.5096 - type_sparse_categorical_accuracy: 0.5828 - val_loss: 5.9020 - val_price_loss: 4.0591 - val_type_loss: 7.7449 - val_price_sparse_categorical_accuracy: 0.5636 - val_type_sparse_categorical_accuracy: 0.6536\n",
            "Epoch 4/55\n",
            "534/534 [==============================] - 18s 33ms/step - loss: 10.7747 - price_loss: 8.2552 - type_loss: 13.2941 - price_sparse_categorical_accuracy: 0.5032 - type_sparse_categorical_accuracy: 0.5890 - val_loss: 13.4365 - val_price_loss: 7.8003 - val_type_loss: 19.0726 - val_price_sparse_categorical_accuracy: 0.4216 - val_type_sparse_categorical_accuracy: 0.4557\n",
            "Epoch 5/55\n",
            "534/534 [==============================] - 18s 34ms/step - loss: 9.5469 - price_loss: 7.2072 - type_loss: 11.8866 - price_sparse_categorical_accuracy: 0.5052 - type_sparse_categorical_accuracy: 0.5991 - val_loss: 8.3792 - val_price_loss: 4.6071 - val_type_loss: 12.1513 - val_price_sparse_categorical_accuracy: 0.4539 - val_type_sparse_categorical_accuracy: 0.5474\n",
            "Epoch 6/55\n",
            "534/534 [==============================] - 18s 34ms/step - loss: 11.7397 - price_loss: 7.9024 - type_loss: 15.5771 - price_sparse_categorical_accuracy: 0.5184 - type_sparse_categorical_accuracy: 0.6079 - val_loss: 8.1742 - val_price_loss: 6.1077 - val_type_loss: 10.2408 - val_price_sparse_categorical_accuracy: 0.5177 - val_type_sparse_categorical_accuracy: 0.6531\n",
            "Epoch 7/55\n",
            "534/534 [==============================] - 18s 34ms/step - loss: 14.4632 - price_loss: 10.3812 - type_loss: 18.5453 - price_sparse_categorical_accuracy: 0.5292 - type_sparse_categorical_accuracy: 0.6368 - val_loss: 10.5093 - val_price_loss: 6.2210 - val_type_loss: 14.7975 - val_price_sparse_categorical_accuracy: 0.5911 - val_type_sparse_categorical_accuracy: 0.5976\n",
            "Epoch 8/55\n",
            "534/534 [==============================] - 18s 34ms/step - loss: 8.7131 - price_loss: 5.9907 - type_loss: 11.4354 - price_sparse_categorical_accuracy: 0.5399 - type_sparse_categorical_accuracy: 0.6772 - val_loss: 10.6009 - val_price_loss: 6.2137 - val_type_loss: 14.9881 - val_price_sparse_categorical_accuracy: 0.5007 - val_type_sparse_categorical_accuracy: 0.5566\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from numpy.random import seed\n",
        "seed(3)\n",
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=55,\n",
        "    batch_size=10,\n",
        "    validation_split=0.3,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5 )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M97px3KFGROz"
      },
      "source": [
        "# Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb830b3-39dc-4cdc-bc23-27578952c983",
        "id": "sEB0Fk1YlXSY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9.9975652e-01 2.4347439e-04 9.7694817e-22]\n",
            " [2.6685420e-01 7.3314577e-01 2.3207889e-21]\n",
            " [6.6291600e-01 3.3708405e-01 7.4951723e-10]\n",
            " ...\n",
            " [9.3441552e-01 6.5584503e-02 2.5083228e-25]\n",
            " [1.1528161e-03 9.9883360e-01 1.3599216e-05]\n",
            " [9.9998724e-01 1.1975956e-05 8.3998418e-07]]\n",
            "[0 1 0 ... 0 1 0]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted = y_predict['price']\n",
        "print(price_predicted)\n",
        "\n",
        "# categories\n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "print(price_category_predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Dc4yoiJlXSa"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': df1.index,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('sample_submissionTrial4.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Public score: 0.49836\n",
        "0.48668"
      ],
      "metadata": {
        "id": "13dvIzoZwZAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that GRU gives us less performance than LSTM as GRU uses less training parameter and therefore uses less memory and executes faster than LSTM whereas LSTM is more accurate on a larger dataset as in our case."
      ],
      "metadata": {
        "id": "QYay5_CF-WfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will try using dropout with simple 2 layer convolutional network for image and LSTM for word part with small numbers of epochs"
      ],
      "metadata": {
        "id": "bp3e_EOi-i1-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRbp1uf2GRO0"
      },
      "source": [
        "# Trial 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIWIA3ZbGRO0",
        "outputId": "8a9b8b32-ef12-4613-98ca-c2d569cb4776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 49, 49, 32)   16416       ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 42, 42, 13)   26637       ['conv2d_11[0][0]']              \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 100, 100)     4000000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_6 (MaxPooling2D)  (None, 2, 2, 13)    0           ['conv2d_12[0][0]']              \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  (None, 100)          80400       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 2, 2, 13)     0           ['max_pooling2d_6[0][0]']        \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 100)          0           ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            " flatten_5 (Flatten)            (None, 52)           0           ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " tf.concat_4 (TFOpLambda)       (None, 152)          0           ['dropout_8[0][0]',              \n",
            "                                                                  'flatten_5[0][0]']              \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            459         ['tf.concat_4[0][0]']            \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           3672        ['tf.concat_4[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,127,584\n",
            "Trainable params: 4,127,584\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Bidirectional\n",
        "\n",
        "from tensorflow.keras.applications import VGG19\n",
        "# image part \n",
        "# simple conv2d. you can change it to anything else as needed\n",
        "\n",
        "cov = Conv2D(32, (16, 16))(in_image)\n",
        "cov1 = Conv2D(13, (8, 8))(cov)\n",
        "# VGG=VGG19(input_tensor=in_image, pooling='avg')\n",
        "pl = MaxPool2D((16, 16))(cov1)\n",
        "d_= Dropout(0.2)(pl)\n",
        "\n",
        "flattened = Flatten()(d_)\n",
        "\n",
        "# word part\n",
        "w=LSTM(100)(embedded)\n",
        "d1=Dropout(0.6)(w)\n",
        "\n",
        "# fusion - combinig both\n",
        "fused = tf.concat([d1, flattened], axis=-1)\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "# define model input/output using keys.\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summery': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.5,\n",
        "        'price': 0.5,       \n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIT9riRDGRO1"
      },
      "source": [
        "# Model Training\n",
        "Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90d37a7d-24d1-4a33-9588-b64b0c98f982",
        "id": "jW5jVblkhRGD"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "382/382 [==============================] - 24s 53ms/step - loss: 13.4305 - price_loss: 12.1350 - type_loss: 14.7260 - price_sparse_categorical_accuracy: 0.4963 - type_sparse_categorical_accuracy: 0.5566 - val_loss: 4.2476 - val_price_loss: 2.3424 - val_type_loss: 6.1528 - val_price_sparse_categorical_accuracy: 0.5872 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 2/20\n",
            "382/382 [==============================] - 19s 49ms/step - loss: 3.1730 - price_loss: 3.0210 - type_loss: 3.3250 - price_sparse_categorical_accuracy: 0.5173 - type_sparse_categorical_accuracy: 0.5927 - val_loss: 2.0282 - val_price_loss: 2.3743 - val_type_loss: 1.6822 - val_price_sparse_categorical_accuracy: 0.6147 - val_type_sparse_categorical_accuracy: 0.6540\n",
            "Epoch 3/20\n",
            "382/382 [==============================] - 17s 44ms/step - loss: 2.1003 - price_loss: 1.9566 - type_loss: 2.2441 - price_sparse_categorical_accuracy: 0.5224 - type_sparse_categorical_accuracy: 0.6268 - val_loss: 1.5111 - val_price_loss: 1.3920 - val_type_loss: 1.6302 - val_price_sparse_categorical_accuracy: 0.5760 - val_type_sparse_categorical_accuracy: 0.7621\n",
            "Epoch 4/20\n",
            "382/382 [==============================] - 12s 32ms/step - loss: 2.0202 - price_loss: 1.8052 - type_loss: 2.2353 - price_sparse_categorical_accuracy: 0.5234 - type_sparse_categorical_accuracy: 0.6253 - val_loss: 2.6344 - val_price_loss: 1.8770 - val_type_loss: 3.3917 - val_price_sparse_categorical_accuracy: 0.3585 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 5/20\n",
            "382/382 [==============================] - 12s 32ms/step - loss: 1.6138 - price_loss: 1.3758 - type_loss: 1.8518 - price_sparse_categorical_accuracy: 0.5442 - type_sparse_categorical_accuracy: 0.6820 - val_loss: 1.0125 - val_price_loss: 0.8925 - val_type_loss: 1.1325 - val_price_sparse_categorical_accuracy: 0.6009 - val_type_sparse_categorical_accuracy: 0.7628\n",
            "Epoch 6/20\n",
            "382/382 [==============================] - 12s 32ms/step - loss: 1.1249 - price_loss: 0.9881 - type_loss: 1.2617 - price_sparse_categorical_accuracy: 0.5622 - type_sparse_categorical_accuracy: 0.7241 - val_loss: 1.0229 - val_price_loss: 0.8990 - val_type_loss: 1.1468 - val_price_sparse_categorical_accuracy: 0.5937 - val_type_sparse_categorical_accuracy: 0.7582\n",
            "Epoch 7/20\n",
            "382/382 [==============================] - 12s 32ms/step - loss: 1.0768 - price_loss: 0.9406 - type_loss: 1.2129 - price_sparse_categorical_accuracy: 0.5717 - type_sparse_categorical_accuracy: 0.7353 - val_loss: 0.9765 - val_price_loss: 0.8795 - val_type_loss: 1.0734 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7641\n",
            "Epoch 8/20\n",
            "382/382 [==============================] - 12s 31ms/step - loss: 1.0289 - price_loss: 0.8928 - type_loss: 1.1650 - price_sparse_categorical_accuracy: 0.5956 - type_sparse_categorical_accuracy: 0.7350 - val_loss: 0.9582 - val_price_loss: 0.8626 - val_type_loss: 1.0538 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7674\n",
            "Epoch 9/20\n",
            "382/382 [==============================] - 12s 32ms/step - loss: 1.0373 - price_loss: 0.8969 - type_loss: 1.1777 - price_sparse_categorical_accuracy: 0.5884 - type_sparse_categorical_accuracy: 0.7381 - val_loss: 0.9617 - val_price_loss: 0.8488 - val_type_loss: 1.0746 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 10/20\n",
            "382/382 [==============================] - 13s 34ms/step - loss: 1.0303 - price_loss: 0.8772 - type_loss: 1.1833 - price_sparse_categorical_accuracy: 0.6004 - type_sparse_categorical_accuracy: 0.7405 - val_loss: 0.9515 - val_price_loss: 0.8383 - val_type_loss: 1.0647 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 11/20\n",
            "382/382 [==============================] - 13s 33ms/step - loss: 66.2577 - price_loss: 38.4388 - type_loss: 94.0767 - price_sparse_categorical_accuracy: 0.5573 - type_sparse_categorical_accuracy: 0.6823 - val_loss: 81.7617 - val_price_loss: 56.5415 - val_type_loss: 106.9820 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 12/20\n",
            "382/382 [==============================] - 12s 32ms/step - loss: 12.6522 - price_loss: 6.1825 - type_loss: 19.1219 - price_sparse_categorical_accuracy: 0.5093 - type_sparse_categorical_accuracy: 0.5933 - val_loss: 1.5512 - val_price_loss: 0.9710 - val_type_loss: 2.1314 - val_price_sparse_categorical_accuracy: 0.5629 - val_type_sparse_categorical_accuracy: 0.6966\n",
            "Epoch 13/20\n",
            "382/382 [==============================] - 12s 32ms/step - loss: 1.4698 - price_loss: 0.9498 - type_loss: 1.9899 - price_sparse_categorical_accuracy: 0.5973 - type_sparse_categorical_accuracy: 0.7004 - val_loss: 1.0521 - val_price_loss: 0.8563 - val_type_loss: 1.2480 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 14/20\n",
            "382/382 [==============================] - 12s 32ms/step - loss: 1.0716 - price_loss: 0.8464 - type_loss: 1.2968 - price_sparse_categorical_accuracy: 0.6165 - type_sparse_categorical_accuracy: 0.7410 - val_loss: 1.0557 - val_price_loss: 0.8306 - val_type_loss: 1.2807 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7654\n",
            "Epoch 15/20\n",
            "382/382 [==============================] - 12s 32ms/step - loss: 1.0372 - price_loss: 0.8427 - type_loss: 1.2318 - price_sparse_categorical_accuracy: 0.6197 - type_sparse_categorical_accuracy: 0.7443 - val_loss: 0.9616 - val_price_loss: 0.8286 - val_type_loss: 1.0947 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7654\n",
            "Epoch 16/20\n",
            "382/382 [==============================] - 12s 32ms/step - loss: 0.9879 - price_loss: 0.8306 - type_loss: 1.1451 - price_sparse_categorical_accuracy: 0.6217 - type_sparse_categorical_accuracy: 0.7479 - val_loss: 0.9893 - val_price_loss: 0.8514 - val_type_loss: 1.1273 - val_price_sparse_categorical_accuracy: 0.6140 - val_type_sparse_categorical_accuracy: 0.7661\n",
            "Epoch 17/20\n",
            "382/382 [==============================] - 12s 32ms/step - loss: 0.9665 - price_loss: 0.8292 - type_loss: 1.1039 - price_sparse_categorical_accuracy: 0.6194 - type_sparse_categorical_accuracy: 0.7487 - val_loss: 0.9415 - val_price_loss: 0.8387 - val_type_loss: 1.0444 - val_price_sparse_categorical_accuracy: 0.6271 - val_type_sparse_categorical_accuracy: 0.7654\n",
            "Epoch 18/20\n",
            "382/382 [==============================] - 12s 32ms/step - loss: 0.9546 - price_loss: 0.8313 - type_loss: 1.0779 - price_sparse_categorical_accuracy: 0.6158 - type_sparse_categorical_accuracy: 0.7489 - val_loss: 1.0919 - val_price_loss: 0.8611 - val_type_loss: 1.3227 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7661\n",
            "Epoch 19/20\n",
            "382/382 [==============================] - 12s 32ms/step - loss: 150.3939 - price_loss: 85.6958 - type_loss: 215.0922 - price_sparse_categorical_accuracy: 0.5027 - type_sparse_categorical_accuracy: 0.5627 - val_loss: 2.9969 - val_price_loss: 1.2207 - val_type_loss: 4.7730 - val_price_sparse_categorical_accuracy: 0.6258 - val_type_sparse_categorical_accuracy: 0.7556\n",
            "Epoch 20/20\n",
            "382/382 [==============================] - 12s 32ms/step - loss: 2.8950 - price_loss: 1.1798 - type_loss: 4.6101 - price_sparse_categorical_accuracy: 0.5489 - type_sparse_categorical_accuracy: 0.6368 - val_loss: 1.6714 - val_price_loss: 0.8678 - val_type_loss: 2.4751 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7628\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from numpy.random import seed\n",
        "seed(3)\n",
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5 )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwhJ-xkbGRO1"
      },
      "source": [
        "# Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d953e8a-dea5-4d21-ffb2-b44128c92f68",
        "id": "qqZb14C6hmSp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.6814472  0.22605266 0.09250009]\n",
            " [0.69282883 0.22570217 0.08146902]\n",
            " [0.74643165 0.21613705 0.03743126]\n",
            " ...\n",
            " [0.71063715 0.18143041 0.10793239]\n",
            " [0.618037   0.35038212 0.03158092]\n",
            " [0.7430222  0.23661877 0.02035901]]\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted = y_predict['price']\n",
        "print(price_predicted)\n",
        "\n",
        "# categories\n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "print(price_category_predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_Hk8lkwhmSq"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': df1.index,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('sample_submissionTrial5.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Public score: 0.62038\n",
        "0.62961"
      ],
      "metadata": {
        "id": "TsOh6INSwfGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the increase of performance of our model and the decrease of loss after decreasing number of epochs and adding dropout"
      ],
      "metadata": {
        "id": "aoLVjesy--it"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next trial we will try to increace the performace by adding layers"
      ],
      "metadata": {
        "id": "NRsFAJlh_-jl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z4HqNs9GRO2"
      },
      "source": [
        "# Trial 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15tS20xYGRO2",
        "outputId": "4d8d22dd-521c-48e0-e4fd-6b5a56997356"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 49, 49, 32)   16416       ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 32)    0           ['conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 3, 3, 32)     0           ['max_pooling2d_4[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 2, 2, 32)     4128        ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling2d_5 (MaxPooling2D)  (None, 1, 1, 32)    0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 1, 1, 32)     0           ['max_pooling2d_5[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 32)     1056        ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 200)]        0           []                               \n",
            "                                                                                                  \n",
            " max_pooling2d_6 (MaxPooling2D)  (None, 1, 1, 32)    0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 200, 100)     8000000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 1, 1, 32)     0           ['max_pooling2d_6[0][0]']        \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  (None, 60)           38640       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 32)           0           ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " tf.concat_2 (TFOpLambda)       (None, 92)           0           ['lstm_1[0][0]',                 \n",
            "                                                                  'flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            279         ['tf.concat_2[0][0]']            \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           2232        ['tf.concat_2[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,062,751\n",
            "Trainable params: 8,062,751\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Bidirectional\n",
        "\n",
        "# image part \n",
        "# simple conv2d. you can change it to anything else as needed\n",
        "cov1 = Conv2D(32, (16, 16))(in_image)\n",
        "pl1 = MaxPool2D((16, 16))(cov1)\n",
        "dropout1 = tf.keras.layers.Dropout(0.6)(pl1)\n",
        "\n",
        "cov2 = Conv2D(32, (2, 2))(dropout1)\n",
        "pl2 = MaxPool2D((2, 2))(cov2)\n",
        "dropout2 = tf.keras.layers.Dropout(0.6)(pl2)\n",
        "\n",
        "cov3 = Conv2D(32, (1, 1))(dropout2)\n",
        "pl3 = MaxPool2D((1, 1))(cov3)\n",
        "dropout3 = tf.keras.layers.Dropout(0.6)(pl3)\n",
        "\n",
        "flattened = Flatten()(dropout3)\n",
        "\n",
        "# word part\n",
        "w=LSTM(60)(embedded)\n",
        "\n",
        "# embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
        "# averaged = tf.reduce_mean(embedded, axis=1)\n",
        "\n",
        "# fusion - combinig both\n",
        "fused = tf.concat([w, flattened], axis=-1)\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "# define model input/output using keys.\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summery': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.5,\n",
        "        'price': 0.5,       \n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnaZuvibGRO2"
      },
      "source": [
        "## Model Training\n",
        "Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on price prediction (assuming that it is the main task we would like to perform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc8a7c41-8666-458c-ae8a-489888e598d1",
        "id": "Cba1fEkyfTa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "382/382 [==============================] - 7s 16ms/step - loss: 62.1077 - price_loss: 43.5912 - type_loss: 80.6242 - price_sparse_categorical_accuracy: 0.4616 - type_sparse_categorical_accuracy: 0.4047 - val_loss: 12.7770 - val_price_loss: 7.6453 - val_type_loss: 17.9086 - val_price_sparse_categorical_accuracy: 0.0898 - val_type_sparse_categorical_accuracy: 0.0223\n",
            "Epoch 2/20\n",
            "382/382 [==============================] - 6s 15ms/step - loss: 10.8936 - price_loss: 4.6652 - type_loss: 17.1221 - price_sparse_categorical_accuracy: 0.5373 - type_sparse_categorical_accuracy: 0.5475 - val_loss: 8.2091 - val_price_loss: 1.2342 - val_type_loss: 15.1841 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.0308\n",
            "Epoch 3/20\n",
            "382/382 [==============================] - 6s 15ms/step - loss: 4.0160 - price_loss: 1.4705 - type_loss: 6.5614 - price_sparse_categorical_accuracy: 0.5724 - type_sparse_categorical_accuracy: 0.6351 - val_loss: 1.3257 - val_price_loss: 1.0106 - val_type_loss: 1.6408 - val_price_sparse_categorical_accuracy: 0.6442 - val_type_sparse_categorical_accuracy: 0.7641\n",
            "Epoch 4/20\n",
            "382/382 [==============================] - 6s 15ms/step - loss: 2.3850 - price_loss: 0.9718 - type_loss: 3.7982 - price_sparse_categorical_accuracy: 0.5802 - type_sparse_categorical_accuracy: 0.6817 - val_loss: 1.1480 - val_price_loss: 0.7838 - val_type_loss: 1.5121 - val_price_sparse_categorical_accuracy: 0.6337 - val_type_sparse_categorical_accuracy: 0.7045\n",
            "Epoch 5/20\n",
            "382/382 [==============================] - 6s 15ms/step - loss: 1.7592 - price_loss: 0.8952 - type_loss: 2.6232 - price_sparse_categorical_accuracy: 0.5983 - type_sparse_categorical_accuracy: 0.7107 - val_loss: 1.0892 - val_price_loss: 0.7724 - val_type_loss: 1.4060 - val_price_sparse_categorical_accuracy: 0.6415 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 6/20\n",
            "382/382 [==============================] - 5s 14ms/step - loss: 1.3945 - price_loss: 0.8551 - type_loss: 1.9339 - price_sparse_categorical_accuracy: 0.6112 - type_sparse_categorical_accuracy: 0.7243 - val_loss: 0.9903 - val_price_loss: 0.7548 - val_type_loss: 1.2257 - val_price_sparse_categorical_accuracy: 0.6560 - val_type_sparse_categorical_accuracy: 0.7425\n",
            "Epoch 7/20\n",
            "382/382 [==============================] - 6s 15ms/step - loss: 1.2394 - price_loss: 0.8163 - type_loss: 1.6625 - price_sparse_categorical_accuracy: 0.6320 - type_sparse_categorical_accuracy: 0.7291 - val_loss: 0.9163 - val_price_loss: 0.7424 - val_type_loss: 1.0902 - val_price_sparse_categorical_accuracy: 0.6586 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 8/20\n",
            "382/382 [==============================] - 7s 19ms/step - loss: 1.1330 - price_loss: 0.8007 - type_loss: 1.4653 - price_sparse_categorical_accuracy: 0.6483 - type_sparse_categorical_accuracy: 0.7327 - val_loss: 0.8943 - val_price_loss: 0.7517 - val_type_loss: 1.0369 - val_price_sparse_categorical_accuracy: 0.6547 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 9/20\n",
            "382/382 [==============================] - 10s 26ms/step - loss: 1.0989 - price_loss: 0.8165 - type_loss: 1.3814 - price_sparse_categorical_accuracy: 0.6435 - type_sparse_categorical_accuracy: 0.7291 - val_loss: 0.8678 - val_price_loss: 0.7220 - val_type_loss: 1.0136 - val_price_sparse_categorical_accuracy: 0.6848 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 10/20\n",
            "382/382 [==============================] - 9s 23ms/step - loss: 0.9733 - price_loss: 0.7536 - type_loss: 1.1930 - price_sparse_categorical_accuracy: 0.6710 - type_sparse_categorical_accuracy: 0.7405 - val_loss: 0.8526 - val_price_loss: 0.7150 - val_type_loss: 0.9901 - val_price_sparse_categorical_accuracy: 0.6874 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 11/20\n",
            "382/382 [==============================] - 9s 24ms/step - loss: 0.9038 - price_loss: 0.7237 - type_loss: 1.0840 - price_sparse_categorical_accuracy: 0.6807 - type_sparse_categorical_accuracy: 0.7494 - val_loss: 0.8345 - val_price_loss: 0.7230 - val_type_loss: 0.9460 - val_price_sparse_categorical_accuracy: 0.6697 - val_type_sparse_categorical_accuracy: 0.7641\n",
            "Epoch 12/20\n",
            "382/382 [==============================] - 7s 18ms/step - loss: 0.8620 - price_loss: 0.7097 - type_loss: 1.0144 - price_sparse_categorical_accuracy: 0.6905 - type_sparse_categorical_accuracy: 0.7545 - val_loss: 0.8089 - val_price_loss: 0.7075 - val_type_loss: 0.9103 - val_price_sparse_categorical_accuracy: 0.6828 - val_type_sparse_categorical_accuracy: 0.7654\n",
            "Epoch 13/20\n",
            "382/382 [==============================] - 6s 15ms/step - loss: 0.8305 - price_loss: 0.6881 - type_loss: 0.9729 - price_sparse_categorical_accuracy: 0.6996 - type_sparse_categorical_accuracy: 0.7527 - val_loss: 0.8004 - val_price_loss: 0.7288 - val_type_loss: 0.8721 - val_price_sparse_categorical_accuracy: 0.6743 - val_type_sparse_categorical_accuracy: 0.7713\n",
            "Epoch 14/20\n",
            "382/382 [==============================] - 5s 14ms/step - loss: 0.8110 - price_loss: 0.6791 - type_loss: 0.9430 - price_sparse_categorical_accuracy: 0.7069 - type_sparse_categorical_accuracy: 0.7550 - val_loss: 0.7897 - val_price_loss: 0.7061 - val_type_loss: 0.8732 - val_price_sparse_categorical_accuracy: 0.6841 - val_type_sparse_categorical_accuracy: 0.7720\n",
            "Epoch 15/20\n",
            "382/382 [==============================] - 6s 15ms/step - loss: 0.7750 - price_loss: 0.6615 - type_loss: 0.8885 - price_sparse_categorical_accuracy: 0.7202 - type_sparse_categorical_accuracy: 0.7609 - val_loss: 0.7651 - val_price_loss: 0.6953 - val_type_loss: 0.8350 - val_price_sparse_categorical_accuracy: 0.6953 - val_type_sparse_categorical_accuracy: 0.7785\n",
            "Epoch 16/20\n",
            "382/382 [==============================] - 5s 14ms/step - loss: 0.7386 - price_loss: 0.6444 - type_loss: 0.8329 - price_sparse_categorical_accuracy: 0.7230 - type_sparse_categorical_accuracy: 0.7713 - val_loss: 0.7686 - val_price_loss: 0.7080 - val_type_loss: 0.8293 - val_price_sparse_categorical_accuracy: 0.6940 - val_type_sparse_categorical_accuracy: 0.7890\n",
            "Epoch 17/20\n",
            "382/382 [==============================] - 6s 14ms/step - loss: 0.7287 - price_loss: 0.6376 - type_loss: 0.8197 - price_sparse_categorical_accuracy: 0.7261 - type_sparse_categorical_accuracy: 0.7746 - val_loss: 0.7527 - val_price_loss: 0.6955 - val_type_loss: 0.8099 - val_price_sparse_categorical_accuracy: 0.6927 - val_type_sparse_categorical_accuracy: 0.7824\n",
            "Epoch 18/20\n",
            "382/382 [==============================] - 5s 14ms/step - loss: 0.7157 - price_loss: 0.6252 - type_loss: 0.8063 - price_sparse_categorical_accuracy: 0.7381 - type_sparse_categorical_accuracy: 0.7782 - val_loss: 0.7532 - val_price_loss: 0.6986 - val_type_loss: 0.8079 - val_price_sparse_categorical_accuracy: 0.6907 - val_type_sparse_categorical_accuracy: 0.7857\n",
            "Epoch 19/20\n",
            "382/382 [==============================] - 5s 14ms/step - loss: 0.6659 - price_loss: 0.5918 - type_loss: 0.7401 - price_sparse_categorical_accuracy: 0.7504 - type_sparse_categorical_accuracy: 0.7925 - val_loss: 0.7449 - val_price_loss: 0.6992 - val_type_loss: 0.7905 - val_price_sparse_categorical_accuracy: 0.6927 - val_type_sparse_categorical_accuracy: 0.7896\n",
            "Epoch 20/20\n",
            "382/382 [==============================] - 5s 14ms/step - loss: 0.6478 - price_loss: 0.5808 - type_loss: 0.7148 - price_sparse_categorical_accuracy: 0.7550 - type_sparse_categorical_accuracy: 0.8051 - val_loss: 0.7390 - val_price_loss: 0.7026 - val_type_loss: 0.7754 - val_price_sparse_categorical_accuracy: 0.6874 - val_type_sparse_categorical_accuracy: 0.7949\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from numpy.random import seed\n",
        "seed(3)\n",
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "   epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=6, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiILIQOzGRO3"
      },
      "source": [
        "# Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f018fceb-b98a-4ee6-a377-35e98254d4c3",
        "id": "WIzu8N-Jfe6J"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.7701646  0.19308382 0.03675155]\n",
            " [0.88730407 0.09542233 0.01727364]\n",
            " [0.8727681  0.0934236  0.03380828]\n",
            " ...\n",
            " [0.79087275 0.18642206 0.02270519]\n",
            " [0.98246366 0.0132561  0.0042802 ]\n",
            " [0.7747721  0.20118429 0.02404367]]\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted = y_predict['price']\n",
        "print(price_predicted)\n",
        "\n",
        "# categories\n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "print(price_category_predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biVB6xAEGRO3"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(\n",
        "    {'id': df1.id,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('sample_submissionTrial6.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Public score : 0.67826\n",
        "0.69347"
      ],
      "metadata": {
        "id": "B80imFBMwuwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the increase of performance of our model and the decrease of loss after decreasing number of epochs and adding dropout and the performance of the model increases as we increase more layers"
      ],
      "metadata": {
        "id": "Fwtj9c8J_ze_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trial 7"
      ],
      "metadata": {
        "id": "G3De9bkwKMpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will try transfer learning (VGG)"
      ],
      "metadata": {
        "id": "_CjVU7iWcbvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG19 is a variant of VGG model which in short consists of 19 layers (16 convolution layers, 3 Fully connected layer, 5 MaxPool layers and 1 SoftMax layer)"
      ],
      "metadata": {
        "id": "X0LzCblPANgN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It87wd7_DXnn",
        "outputId": "94ebc34b-1f72-4513-efdb-6cd1ac587eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 49, 49, 10)   5130        ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 100, 100)     4000000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " vgg19 (Functional)             (None, 1, 1, 512)    20028416    ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpLambd  (None, 100)         0           ['embedding[0][0]']              \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 512)          0           ['vgg19[0][0]']                  \n",
            "                                                                                                  \n",
            " tf.concat (TFOpLambda)         (None, 612)          0           ['tf.math.reduce_mean[0][0]',    \n",
            "                                                                  'flatten[0][0]']                \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            1839        ['tf.concat[0][0]']              \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           14712       ['tf.concat[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 24,050,097\n",
            "Trainable params: 24,050,097\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "# here we have two inputs. one for image and the other for text.\n",
        "in_text = keras.Input(batch_shape=(None, max_len))\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "# text part\n",
        "# simple average of embedding.\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
        "averaged = tf.reduce_mean(embedded, axis=1)\n",
        "\n",
        "\n",
        "# image part \n",
        "# simple conv2d. \n",
        "cov1 = Conv2D(10,(16,16), activation='tanh')(in_image) # 10 number of filters  and  (15, 15) size of filter\n",
        "vgg=VGG19(weights=None, input_shape=(49, 49, 10), include_top=False)(cov1)\n",
        "# con_drop = Dropout(.2)(vgg)\n",
        "flattened = Flatten()(vgg)\n",
        "\n",
        "# fusion - combinig both\n",
        "fused = tf.concat([averaged, flattened], axis=-1)\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "\n",
        "\n",
        "# define model input/output using keys.\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.7,\n",
        "        'price': 0.3,       \n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAyGx2lTDXqg",
        "outputId": "2ac49683-7cca-468d-a2e5-00b11789e0c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "339/339 [==============================] - 50s 114ms/step - loss: 3.2657 - price_loss: 2.3558 - type_loss: 3.6556 - price_sparse_categorical_accuracy: 0.5999 - type_sparse_categorical_accuracy: 0.7514 - val_loss: 0.9251 - val_price_loss: 0.8299 - val_type_loss: 0.9658 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 2/15\n",
            "339/339 [==============================] - 37s 110ms/step - loss: 0.9328 - price_loss: 0.8041 - type_loss: 0.9879 - price_sparse_categorical_accuracy: 0.6260 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.8782 - val_price_loss: 0.7782 - val_type_loss: 0.9210 - val_price_sparse_categorical_accuracy: 0.6448 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 3/15\n",
            "339/339 [==============================] - 38s 111ms/step - loss: 0.8661 - price_loss: 0.7559 - type_loss: 0.9134 - price_sparse_categorical_accuracy: 0.6573 - type_sparse_categorical_accuracy: 0.7538 - val_loss: 0.8302 - val_price_loss: 0.7436 - val_type_loss: 0.8673 - val_price_sparse_categorical_accuracy: 0.6645 - val_type_sparse_categorical_accuracy: 0.7654\n",
            "Epoch 4/15\n",
            "339/339 [==============================] - 37s 111ms/step - loss: 0.8077 - price_loss: 0.7194 - type_loss: 0.8455 - price_sparse_categorical_accuracy: 0.6769 - type_sparse_categorical_accuracy: 0.7595 - val_loss: 0.8038 - val_price_loss: 0.7237 - val_type_loss: 0.8382 - val_price_sparse_categorical_accuracy: 0.6782 - val_type_sparse_categorical_accuracy: 0.7733\n",
            "Epoch 5/15\n",
            "339/339 [==============================] - 37s 110ms/step - loss: 0.7584 - price_loss: 0.6916 - type_loss: 0.7871 - price_sparse_categorical_accuracy: 0.6928 - type_sparse_categorical_accuracy: 0.7748 - val_loss: 0.7787 - val_price_loss: 0.7152 - val_type_loss: 0.8060 - val_price_sparse_categorical_accuracy: 0.6809 - val_type_sparse_categorical_accuracy: 0.7798\n",
            "Epoch 6/15\n",
            "339/339 [==============================] - 37s 110ms/step - loss: 0.7164 - price_loss: 0.6688 - type_loss: 0.7368 - price_sparse_categorical_accuracy: 0.7069 - type_sparse_categorical_accuracy: 0.7876 - val_loss: 0.7647 - val_price_loss: 0.7073 - val_type_loss: 0.7893 - val_price_sparse_categorical_accuracy: 0.6913 - val_type_sparse_categorical_accuracy: 0.7857\n",
            "Epoch 7/15\n",
            "339/339 [==============================] - 37s 110ms/step - loss: 0.6782 - price_loss: 0.6492 - type_loss: 0.6906 - price_sparse_categorical_accuracy: 0.7191 - type_sparse_categorical_accuracy: 0.8002 - val_loss: 0.7502 - val_price_loss: 0.7020 - val_type_loss: 0.7708 - val_price_sparse_categorical_accuracy: 0.6940 - val_type_sparse_categorical_accuracy: 0.7870\n",
            "Epoch 8/15\n",
            "339/339 [==============================] - 37s 110ms/step - loss: 0.6411 - price_loss: 0.6291 - type_loss: 0.6463 - price_sparse_categorical_accuracy: 0.7338 - type_sparse_categorical_accuracy: 0.8117 - val_loss: 0.7427 - val_price_loss: 0.7002 - val_type_loss: 0.7609 - val_price_sparse_categorical_accuracy: 0.6959 - val_type_sparse_categorical_accuracy: 0.7857\n",
            "Epoch 9/15\n",
            "339/339 [==============================] - 37s 110ms/step - loss: 0.6091 - price_loss: 0.6108 - type_loss: 0.6083 - price_sparse_categorical_accuracy: 0.7407 - type_sparse_categorical_accuracy: 0.8246 - val_loss: 0.7362 - val_price_loss: 0.6979 - val_type_loss: 0.7526 - val_price_sparse_categorical_accuracy: 0.6986 - val_type_sparse_categorical_accuracy: 0.7883\n",
            "Epoch 10/15\n",
            "339/339 [==============================] - 37s 110ms/step - loss: 0.5786 - price_loss: 0.5924 - type_loss: 0.5726 - price_sparse_categorical_accuracy: 0.7546 - type_sparse_categorical_accuracy: 0.8379 - val_loss: 0.7354 - val_price_loss: 0.6981 - val_type_loss: 0.7515 - val_price_sparse_categorical_accuracy: 0.7005 - val_type_sparse_categorical_accuracy: 0.7962\n",
            "Epoch 11/15\n",
            "339/339 [==============================] - 40s 117ms/step - loss: 0.5504 - price_loss: 0.5765 - type_loss: 0.5392 - price_sparse_categorical_accuracy: 0.7641 - type_sparse_categorical_accuracy: 0.8485 - val_loss: 0.7364 - val_price_loss: 0.6984 - val_type_loss: 0.7528 - val_price_sparse_categorical_accuracy: 0.6986 - val_type_sparse_categorical_accuracy: 0.7969\n",
            "Epoch 12/15\n",
            "339/339 [==============================] - 37s 110ms/step - loss: 0.5243 - price_loss: 0.5602 - type_loss: 0.5089 - price_sparse_categorical_accuracy: 0.7705 - type_sparse_categorical_accuracy: 0.8620 - val_loss: 0.7353 - val_price_loss: 0.7033 - val_type_loss: 0.7490 - val_price_sparse_categorical_accuracy: 0.6959 - val_type_sparse_categorical_accuracy: 0.7962\n",
            "Epoch 13/15\n",
            "339/339 [==============================] - 40s 117ms/step - loss: 0.4996 - price_loss: 0.5449 - type_loss: 0.4802 - price_sparse_categorical_accuracy: 0.7776 - type_sparse_categorical_accuracy: 0.8692 - val_loss: 0.7347 - val_price_loss: 0.7024 - val_type_loss: 0.7485 - val_price_sparse_categorical_accuracy: 0.6986 - val_type_sparse_categorical_accuracy: 0.8008\n",
            "Epoch 14/15\n",
            "339/339 [==============================] - 37s 110ms/step - loss: 0.4763 - price_loss: 0.5296 - type_loss: 0.4535 - price_sparse_categorical_accuracy: 0.7876 - type_sparse_categorical_accuracy: 0.8764 - val_loss: 0.7438 - val_price_loss: 0.7051 - val_type_loss: 0.7604 - val_price_sparse_categorical_accuracy: 0.6953 - val_type_sparse_categorical_accuracy: 0.7969\n",
            "Epoch 15/15\n",
            "339/339 [==============================] - 37s 110ms/step - loss: 0.4540 - price_loss: 0.5139 - type_loss: 0.4283 - price_sparse_categorical_accuracy: 0.7950 - type_sparse_categorical_accuracy: 0.8846 - val_loss: 0.7468 - val_price_loss: 0.7116 - val_type_loss: 0.7618 - val_price_sparse_categorical_accuracy: 0.6946 - val_type_sparse_categorical_accuracy: 0.7995\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from numpy.random import seed\n",
        "seed(3)\n",
        "history = model.fit(\n",
        "    x={\n",
        "        'summery': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "       \n",
        "    epochs=15,\n",
        "    batch_size=18,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=6, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwuGB9mwEIcY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a599f0e-0391-463e-b21f-b9b92dc75027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.6951705  0.23210998 0.07271954]\n",
            " [0.8859279  0.09371065 0.02036142]\n",
            " [0.864078   0.09438037 0.04154169]\n",
            " ...\n",
            " [0.7646815  0.20048192 0.03483657]\n",
            " [0.98987997 0.00791429 0.00220573]\n",
            " [0.78921485 0.1814672  0.02931791]]\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text_id,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted = y_predict['price']\n",
        "print(price_predicted)\n",
        "\n",
        "# categories\n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "print(price_category_predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ6MBC9LEMng"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': df1.index,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('trial.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Public score: 0.67853\n",
        "0.69239\n",
        "##### *Optimal solution*"
      ],
      "metadata": {
        "id": "hVKMoxL6xAIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can the that VGG model gave us the best performance"
      ],
      "metadata": {
        "id": "nySDmkSAAd5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next plan, we will try tuning the model using  Neural architecture search instead of manual try and error for choosing the best parameters"
      ],
      "metadata": {
        "id": "m8yb2fhXedB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion (Questions & Answers)\n",
        "#### 🌈Is fully-connected model a good one for sequential data? Why? How about for image data? Is it good? Why?\n",
        "1)No, In case of sequential data, the inputs and outputs can be of different lengths, we map the input sentences to one number describing the sentiment of the text.\n",
        "Standard neural network does not share features learnt across different positions of text. \n",
        "The parameters required for handling text will be very large in case of fully connected model. so it can't handel time series data or sequentaila data \n",
        "2) No,because it doen't share weights and images have many weights that their is no computing ower can handel this and fully connected layer cannot extract features so it gives us bad accuracy.\n",
        "#### 🌈What is gradient vanishing and gradient explosion, and how GRU/LSTM tries to mitigate this problem?\n",
        "Exploding gradient occurs when the derivatives or slope will get larger and larger as we go backward with every layer during backpropagation. This situation is the exact opposite of the vanishing gradients.\n",
        "\n",
        "\n",
        "This problem happens because of weights, not because of the activation function. Due to high weight values, the derivatives will also higher so that the new weight varies a lot to the older weight, and the gradient will never converge. So it may result in oscillating around minima and never come to a global minima point.\n",
        "LSTMs/GRU solve the problem using a unique additive gradient structure that includes direct access to the forget gate's activations, enabling the network to encourage desired behaviour from the error gradient using frequent gates update on every time step of the learning process.\n",
        "#### 🌈What is multi-objective/multi-task learning? What is multi-modality learning? How do you use them in this assignment?\n",
        " Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This may result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately. Multimodal AI is a new AI paradigm, in which various data types (image, text, speech, numerical data) are combined with multiple intelligence processing algorithms to achieve higher performances.\n",
        " In our assignment we had two input features (summary, image) from which we need to predict the type a price so we used the multimodality to be abl to predict two labels\n",
        "#### 🌈What is the difference among xgboost, lightgbm and catboost\n",
        "CatBoost is designed for categorical data and is known to have the best performance on it, showing the state-of-the-art performance over XGBoost and LightGBM in eight datasets in its official journal article. As of CatBoost version 0.6, a trained CatBoost tree can predict extraordinarily faster than either XGBoost or LightGBM.\n",
        "\n",
        "On the flip side, some of CatBoost’s internal identification of categorical data slows its training time significantly in comparison to XGBoost, but it is still reported much faster than XGBoost. LightGBM also boasts accuracy and training speed increases over XGBoost in five of the benchmarks examined in its original publication.\n",
        "XGBoost has been around the block longer than either LightGBM and CatBoost, so it has better learning resources and a more active developer community. It also doesn’t hurt that XGBoost is substantially faster and more accurate than its predecessors and other competitors such as Scikit-learn.\n",
        "XgBoosting\n",
        "• Faster – optimized for multi-threading/processing\n",
        "• + additional regularization\n",
        "• Tree complexity\n",
        "• Structure score\n",
        "• Better handling of missing values\n",
        "LightGBM\n",
        "Even faster (~7-10 times)\n",
        "• Lower memory consumption\n",
        "• Can reach even lower bias on a larger dataset\n",
        "• Higher risk of overfitting\n",
        "\n",
        "Each boosting technique and framework has a time and a place—and it is often not clear which will perform best until testing them all. Fortunately, prior work has done a decent amount of benchmarking the three choices, but ultimately it’s up to you, the engineer, to determine the best tool for the job.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8PEuEVZcCWzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References \n",
        "\n",
        "https://bdtechtalks.com/2022/02/28/what-is-neural-architecture-search/\n",
        "https://www.telusinternational.com/articles/difference-between-cnn-and-rnn\n",
        "https://builtin.com/data-science/recurrent-neural-networks-and-lstm\n",
        "https://medium.com/swlh/fully-connected-vs-convolutional-neural-networks-813ca7bc6ee5\n",
        "https://towardsdatascience.com/comparing-the-performance-of-fully-connected-simple-cnn-and-resnet50-for-binary-image-5dae3cea034\n",
        "https://iq.opengenus.org/fully-connected-layer/#:~:text=Fully%20Connected%20layers%20in%20a,to%20form%20the%20final%20output.\n",
        "https://www.aimesoft.com/multimodalai.html\n",
        "https://www.springboard.com/blog/data-science/xgboost-random-forest-catboost-lightgbm/\n",
        "https://neptune.ai/blog/xgboost-vs-lightgbm\n",
        "https://iq.opengenus.org/vgg19-architecture/#:~:text=VGG19%20is%20a%20variant%20of,VGG19%20has%2019.6%20billion%20FLOPs.\n",
        "https://analyticsindiamag.com/complete-guide-to-bidirectional-lstm-with-python-codes/\n",
        "https://www.analyticsvidhya.com/blog/2021/06/lstm-for-text-classification/"
      ],
      "metadata": {
        "id": "Xw1VFCH1dqTv"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "DMA4.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "efaa064112d943009b374814a6b064d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_917fbdd5fb5847a6ae1ca33c646eabb0",
              "IPY_MODEL_fadb0680530a4175a72f395ee88d43f5",
              "IPY_MODEL_b4d8a32401844d228be5f391f6203417"
            ],
            "layout": "IPY_MODEL_06391be42db445c2989f5fc529694d69"
          }
        },
        "917fbdd5fb5847a6ae1ca33c646eabb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4d2a97ce9ee4adcb9ddaf812d88cc1b",
            "placeholder": "​",
            "style": "IPY_MODEL_b17822ad910c48758b789146ab57a0ee",
            "value": "100%"
          }
        },
        "fadb0680530a4175a72f395ee88d43f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae73e39263344b8392afb1d95e0232b0",
            "max": 7627,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c8831e428424a538647f9c30c98d5aa",
            "value": 7627
          }
        },
        "b4d8a32401844d228be5f391f6203417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbc7de0761534ca9985a8e903e39b6e5",
            "placeholder": "​",
            "style": "IPY_MODEL_0cee21df36b6491eb579b8209af08ed2",
            "value": " 7627/7627 [01:24&lt;00:00, 95.59it/s]"
          }
        },
        "06391be42db445c2989f5fc529694d69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4d2a97ce9ee4adcb9ddaf812d88cc1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b17822ad910c48758b789146ab57a0ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae73e39263344b8392afb1d95e0232b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c8831e428424a538647f9c30c98d5aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbc7de0761534ca9985a8e903e39b6e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cee21df36b6491eb579b8209af08ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25e9d2972e5e45498093b16b75a8097a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3225db3151034fdbae67d1d48a6f5998",
              "IPY_MODEL_495e813760d54697b3a83477ef6d6d7c",
              "IPY_MODEL_a56ec038f22047878ce7ab6864592c38"
            ],
            "layout": "IPY_MODEL_907838a6574d46d7a5f5991d3f6e9c52"
          }
        },
        "3225db3151034fdbae67d1d48a6f5998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e27d67c854d4de5b4a1b20489ed2177",
            "placeholder": "​",
            "style": "IPY_MODEL_cab6fc7bc401417d83709f7dbab8b124",
            "value": "100%"
          }
        },
        "495e813760d54697b3a83477ef6d6d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_521fb36963f64de28df1a5e13ed503e1",
            "max": 7360,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2cbfa36ef0ad46b8b3a121f7cd325975",
            "value": 7360
          }
        },
        "a56ec038f22047878ce7ab6864592c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b58a292b2254aff95dc16962d4a3fcc",
            "placeholder": "​",
            "style": "IPY_MODEL_4054da212a164ba9ae27b410db3f3c62",
            "value": " 7360/7360 [01:19&lt;00:00, 90.73it/s]"
          }
        },
        "907838a6574d46d7a5f5991d3f6e9c52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e27d67c854d4de5b4a1b20489ed2177": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cab6fc7bc401417d83709f7dbab8b124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "521fb36963f64de28df1a5e13ed503e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cbfa36ef0ad46b8b3a121f7cd325975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b58a292b2254aff95dc16962d4a3fcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4054da212a164ba9ae27b410db3f3c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}